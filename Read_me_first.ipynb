{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load_ext autoreload\n",
      "%autoreload 2\n",
      "import pylab as pl\n",
      "%matplotlib inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Background\n",
      "##1. Given\n",
      "$$f\\colon \\mathbb{R}^{n}\\to \\mathbb{R}^{m}\\,\\,\\, , \\,\\,\\, \\vec{x} \\mapsto \\vec{y}=f\\left(\\vec{x},\\vec{p}\\right)$$\n",
      "where $\\vec{x}$ is the state of a system (e.g. $n$ parameter of an atmosphere) and $\\vec{y}$ (e.g. a simulated measurement in $m$ channels) is the result of a forward operator $f$ (e.g. a radiative transfer model). $\\vec{p}$ are additional parameter (e.g. observation geometry)  \n",
      "##2. Wanted\n",
      "$$f^{-1}\\colon \\mathbb{R}^{m}\\to \\mathbb{R}^{n}\\,\\,\\, , \\,\\,\\, \\vec{y} \\mapsto \\vec{x}=f^{-1}\\left(\\vec{y},\\vec{p}\\right)$$\n",
      "where $f^{-1}$ (the inverse of $f$) is the function, that relates a measurement $\\vec{y}$ to a state $\\vec{x}$.\n",
      "##3. Additional Quantities\n",
      "###3.1. Helper $g$ \n",
      " $$g_y = f(x)-y$$ \n",
      "###3.2. Jacobian of $g$ (or $f$) \n",
      "$$K(\\vec{x}):=\\begin{pmatrix}\\frac{\\partial f_{1}}{\\partial x_{1}} & \\frac{\\partial f_{1}}{\\partial x_{2}} & \\ldots & \\frac{\\partial f_{1}}{\\partial x_{n}}\\\\\\\n",
      "\\vdots & \\vdots & \\ddots & \\vdots\\\\\\\n",
      "\\frac{\\partial f_{m}}{\\partial x_{1}} & \\frac{\\partial f_{m}}{\\partial x_{2}} & \\ldots & \\frac{\\partial f_{m}}{\\partial x_{n}}\n",
      "\\end{pmatrix} \\\\\\\n",
      " $$\n",
      "###3.3 Measurement error co-variance \n",
      "$$\n",
      "S_e:=\\begin{pmatrix}  \\sigma^2_{y_1}   &  c\\left(y_1,y_2\\right)\\cdot\\sigma_{y_1}\\sigma_{y_2}      & \\ldots & c\\left(y_1,y_n\\right)\\cdot\\sigma_{y_1}\\sigma_{y_m}\\\\\\\n",
      "                    \\vdots & \\vdots & \\ddots & \\vdots\\\\\\\n",
      " c\\left(y_m,y_1\\right)\\cdot\\sigma_{y_m}\\sigma_{y_1} &  c\\left(y_m,y_2\\right)\\cdot\\sigma_{y_m}\\sigma_{y_2}    & \\ldots & \\sigma^2_{y_m}  \\end{pmatrix} \\\\\\\n",
      " $$\n",
      "###3.4 *apriori* error co-variance\n",
      "$$\n",
      "S_a:=\\begin{pmatrix}  \\sigma^2_{x_{a1}}   &  c\\left(x_{a1},x_{a2}\\right)\\cdot\\sigma_{x_{a1}}\\sigma_{x_{a2}}      & \\ldots & c\\left(x_{a1},x_{an}\\right)\\cdot\\sigma_{x_{a1}}\\sigma_{x_{an}}\\\\\\\n",
      "                    \\vdots & \\vdots & \\ddots & \\vdots\\\\\\\n",
      "c\\left(x_{an},x_{a1}\\right)\\cdot\\sigma_{x_{an}}\\sigma_{x_{a1}} &  c\\left(x_{an},x_{a2}\\right)\\cdot\\sigma_{x_{an}}\\sigma_{x_{a2}}    & \\ldots & \\sigma^2_{x_{an}}  \\end{pmatrix} \\\\\\\n",
      " $$\n",
      "\n",
      "\n",
      "\n",
      "##4. Methods\n",
      "###4.1. Newton Iteration (no additional information is used)\n",
      "####4.1.0. Goal\n",
      "Find root of $g_y\\left(x\\right)$\n",
      "\n",
      "Minimize *cost* $J\\left(\\vec{x}\\right)$  (see diagnostic)\n",
      "####4.1.1. Iteration\n",
      "If the number of measurements is equal to  th number of states ($m = n$) \n",
      "$$\\vec{x}_{i+1}=\\vec{x}_{i} - K_i^{-1} \\cdot g\\left(\\vec{x}_{i}\\right)$$ \n",
      "If the number of measurements is greater than number of states ($m \\gt n$), the left inverse of the Jacobian can be used. \n",
      "$$\\vec{x}_{i+1}=\\vec{x}_{i} - (K_i^{T}K_i)^{-1}\\left[K_i^{T} \\cdot g\\left(\\vec{x}_{i}\\right)\\right]$$\n",
      "####4.1.2. Convergence\n",
      "if\n",
      "$$g\\left(\\vec{x}_{i}\\right)^{2} \\lt \\epsilon_y$$\n",
      "or\n",
      "$$\\left(\\vec{x}_{i} - \\vec{x}_{i+1}\\right)^{2} \\lt \\epsilon_{mach}$$\n",
      "####4.1.3. Diagnostic\n",
      "The *gain*  (the sensitivity of the retrieved $\\hat{\\vec{x}}$ to the observations $\\vec{y}$) is:\n",
      "$$\\\\\n",
      "G:= \\frac{\\partial \\hat{x}}{\\partial y}  = \\begin{pmatrix}\\frac{\\partial \\hat{x}_{1}}{\\partial y_{1}} & \\frac{\\partial \\hat{x}_{1}}{\\partial y_{2}} & \\ldots & \\frac{\\partial \\hat{x}_{1}}{\\partial y_{m}}\\\\\\\n",
      "\\vdots & \\vdots & \\ddots & \\vdots\\\\\\\n",
      "\\frac{\\partial \\hat{x}_{n}}{\\partial y_{1}} & \\frac{\\partial \\hat{x}_{n}}{\\partial y_{2}} & \\ldots & \\frac{\\partial \\hat{x}_{n}}{\\partial y_{m}}\n",
      "\\end{pmatrix}  \\\\$$\n",
      "$\\Rightarrow G = K^{\u207b1}$ or $\\,\\,G =(K_i^{T}K_i)^{-1}K_i^{T} \\\\$\n",
      "\n",
      "The *cost*:\n",
      "$$J\\left(\\vec{x}\\right):=g\\left(\\vec{x}\\right)^T g\\left(\\vec{x}\\right)$$\n",
      "\n",
      "###4.2 Newton Iteration with measurement error co-variance\n",
      "####4.2.0. Goal\n",
      "Minimize *cost* $J\\left(\\vec{x}\\right)$  (see diagnostic)\n",
      "####4.2.1. Iteration\n",
      "$$\\vec{x}_{i+1}=\\vec{x}_{i}-\\left(K_{i}^{T}S_{e}^{-1}K_{i}\\right)^{-1}\\left[K_{i}^{T}S_{e}^{-1}\\cdot g\\left(\\vec{x}_{i}\\right)\\right]$$\n",
      "####4.2.2. Convergence\n",
      "if\n",
      "$$g\\left(\\vec{x}_{i}\\right)^{2} \\lt \\epsilon_y$$\n",
      "or\n",
      "$$\\left(\\vec{x}_{i} - \\vec{x}_{i+1}\\right)^{2} \\lt \\epsilon_{mach}$$\n",
      "or\n",
      "$$\\left(\\vec{x}_{i} - \\vec{x}_{i+1}\\right)^T \\hat{S}^{-1} \\left(\\vec{x}_{i} - \\vec{x}_{i+1}\\right)\\lt \\epsilon_{x}\\cdot n$$\n",
      "using the *retrieval error co-variance* matrix $\\hat{S}$  (see diagnostic)\n",
      "####4.2.3. Diagnostic\n",
      "The *retrieval error co-variance*: \n",
      "$$\\hat{S} = \\left(K_{i}^{T}S_{e}^{-1}K_{i}\\right)^{-1}$$\n",
      "The *gain*:\n",
      "$$G=\\hat{S} \\cdot K^{T}S_{e}^{-1}$$\n",
      "The *cost*:\n",
      "$$J\\left(\\vec{x}\\right):=g\\left(\\vec{x}\\right)^T S_e^{-1}g\\left(\\vec{x}\\right)$$\n",
      "###4.3 Optimal Estimation\n",
      "####4.3.0. Goal\n",
      "-Maximize the *aposteriori* probability (find the state with the highest probability, given a measurement and an *apriori* state $x_a$)   \n",
      "-Minimize *cost*\n",
      "####4.3.1. Iteration\n",
      "$$\\vec{x}_{i+1}=\\vec{x}_{i}-\\left(S_{a}^{-1}+K_{i}^{T}S_{e}^{-1}K_{i}\\right)^{-1}\\left[K_{i}^{T}S_{e}^{-1}\\cdot g\\left(\\vec{x_{i}}\\right)-S_{a}^{-1}\\cdot \\left(\\vec{x}_{a}-\\vec{x_{i}}\\right)\\right]$$\n",
      "####4.3.2. Convergence\n",
      "if\n",
      "$$\\left(\\vec{x}_{i} - \\vec{x}_{i+1}\\right)^{2} \\lt \\epsilon_{mach}$$\n",
      "or\n",
      "$$\\left(\\vec{x}_{i} - \\vec{x}_{i+1}\\right)^T \\hat{S}^{-1} \\left(\\vec{x}_{i} - \\vec{x}_{i+1}\\right)\\lt \\epsilon_{x}\\cdot n$$\n",
      "using the *retrieval error co-variance* matrix $\\hat{S}$\n",
      "####4.3.3. Diagnostic\n",
      "The *retrieval error co-variance*: \n",
      "$$\\hat{S} = \\left(S^{-1}_a + K_{i}^{T}S_{e}^{-1}K_{i}\\right)^{-1} \\\\$$\n",
      "The *gain*:\n",
      "$$G=\\hat{S} \\cdot K^{T}S_{e}^{-1} \\\\$$\n",
      "The *cost*:\n",
      "$$J\\left(\\vec{x}\\right):=\\left(\\vec{x}-\\vec{x}_a\\right)^T S_a^{-1}\\left(\\vec{x}-\\vec{x}_a\\right)+g\\left(\\vec{x}\\right)^T S_e^{-1}g\\left(\\vec{x}\\right)\\\\$$\n",
      "The *averaging kernel* (the sensitivity of the retrieval to the *truth*):\n",
      "$$ \\\\A := \\frac{\\partial \\hat{x}}{\\partial x}  = \\begin{pmatrix}\\frac{\\partial \\hat{x}_{1}}{\\partial x_{1}} & \\frac{\\partial \\hat{x}_{1}}{\\partial x_{2}} & \\ldots & \\frac{\\partial \\hat{x}_{1}}{\\partial x_{n}}\\\\\\\n",
      "\\vdots & \\vdots & \\ddots & \\vdots\\\\\\\n",
      "\\frac{\\partial \\hat{x}_{n}}{\\partial x_{1}} & \\frac{\\partial \\hat{x}_{n}}{\\partial x_{2}} & \\ldots & \\frac{\\partial \\hat{x}_{n}}{\\partial x_{n}}\n",
      "\\end{pmatrix}  = \\frac{\\partial \\hat{x}}{\\partial y} \\cdot \\frac{\\partial y}{\\partial x}= G \\cdot K \\\\$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Application\n",
      "## 1. Forward operator\n",
      "###1.1 Define 'forward' function. \n",
      "This can be *any* kind of python function (e.g. a LUT interpolator), as long as it accepts *x*, *params* and returns *y*. *x* and *y* must be numpy arrays."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "#check all math errors\n",
      "numpy.seterr(all='raise')\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 2,
       "text": [
        "{'divide': 'warn', 'invalid': 'warn', 'over': 'warn', 'under': 'ignore'}"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def func(x,*params):\n",
      "    '''\n",
      "    simple example of a function R^2 --> R^3\n",
      "    (two state variables and three measurements)\n",
      "\n",
      "    In general every function should accept: \n",
      "    Input:\n",
      "           x:  n-element np-array \n",
      "      params:  additional parameter,that are needed (e.g. geometry ..)\n",
      "               could be anything tupel,dict,list,func ....\n",
      "    Output:\n",
      "           y:  m-element  np-array\n",
      "\n",
      "    '''\n",
      "    # this is just a quick and dirty check\n",
      "    # if params can been used\n",
      "    try:\n",
      "        dum=1.**params[0]\n",
      "        exp=params[0]\n",
      "    except:\n",
      "        exp=1.\n",
      "    # simple non-linear useless function \n",
      "    return np.asarray([ np.log(13 + 6*x[0]**exp + 4*x[1] + 0.7*np.power(x[0]*x[1],2))\n",
      "                          , 2 - 3*x[0] + 2*x[1]**exp +   np.sqrt(x[0])*np.log(x[1])\n",
      "                          ,       x[0] - 5*x[1] -   np.sqrt(x[0]*x[1])**exp\n",
      "                          ])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###1.2 Test forward"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x=np.array([1.9,2.8])\n",
      "print func(x)\n",
      "print func(x,1.8)\n",
      "#x0=np.linspace(0.3,53,100)\n",
      "#x1=np.linspace(0.1,10,100)\n",
      "#xx0,xx1=np.meshgrid(x0,x1)\n",
      "#yy=func(np.array([xx0,xx1]))\n",
      "#pl.pcolor(yy[2,:,:].squeeze())\n",
      "#pl.colorbar()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[  4.0147904    3.31923242 -14.40651252]\n",
        "[  4.14412217  10.48110641 -16.60111873]\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##2. Inverse function\n",
      "###Generate inverse function\n",
      "Mandatory are also the lower $(a)$ and the upper $(b)$ bounds "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import optimal_estimation as oe"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# lower limit for x\n",
      "a=np.array([0.3,0.1])\n",
      "# upper limit for x\n",
      "b=np.array([53.,10.])\n",
      "\n",
      "#this creates the inverse function\n",
      "inv_func=oe.my_inverter(func,a,b)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###2.1. Test inverse function  with *pure* Newton"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#measurement\n",
      "y=np.array(func([3.1,4.5]))\n",
      "\n",
      "#Inversion \n",
      "print y,inv_func(y,method=0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[  5.22478788   4.34820153 -23.13496988] [ 3.1  4.5]\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#All diagnostic is returned, if full is set \n",
      "res=inv_func(func([3.4,5.7]),method=0,full=True)\n",
      "for kk in res._fields:  \n",
      "    print kk,'=', getattr(res, kk)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "x = [ 3.4  5.7]\n",
        "j = [[ 0.50345887  0.30161739]\n",
        " [-2.52803567  2.32349311]\n",
        " [ 0.35258759 -5.38616437]]\n",
        "conv = True\n",
        "ni = 4\n",
        "g = [[ 0.11210577 -0.39620005 -0.16463572]\n",
        " [ 0.03350656 -0.02017352 -0.19248705]]\n",
        "a = [[ 1.  0.]\n",
        " [ 0.  1.]]\n",
        "sr = None\n",
        "cost = 3.42141280824e-24\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###2.2. Test inverse with Newton and measurement error covariance"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#measurement error covariance\n",
      "se=np.array([[1,0.,0],[0,1.,0],[0,0,10.]])\n",
      "numpy.seterr(all='raise')\n",
      "\n",
      "# y is a measurement with an error at y[2]\n",
      "y=func(np.array([1.9,2.8]))+np.array([0.,0.,2.0])\n",
      "print y\n",
      "\n",
      "#Inversion\n",
      "print inv_func(y,se=se,method=1,full=True).x\n",
      "print inv_func(y,se=se,method=1)\n",
      "res=inv_func(y,se=se,method=1,full=True)\n",
      "for kk in res._fields:  \n",
      "    print kk,'=', getattr(res, kk)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[  4.0147904    3.31923242 -12.40651252]\n",
        "[ 1.60702053  2.47316564]\n",
        "[ 1.60702053  2.47316564]\n",
        "x"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " = [ 1.60702053  2.47316564]\n",
        "j = [[ 0.45328355  0.29688372]\n",
        " [-2.64280477  2.51257784]\n",
        " [ 0.37963927 -5.40304634]]\n",
        "conv = True\n",
        "ni = 3\n",
        "g = [[ 0.28115819 -0.35141882 -0.14797131]\n",
        " [ 0.23428845  0.01655237 -0.16450987]]\n",
        "a = [[ 1.  0.]\n",
        " [ 0.  1.]]\n",
        "sr = [[ 0.42150021  0.30348271]\n",
        " [ 0.30348271  0.32580003]]\n",
        "cost = 0.0716468150932\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 2.3. Test inverse with *optimal estimation*\n",
      "Needed quantities are:\n",
      "\n",
      " - measurement error covariance \n",
      " - apriori error covariance \n",
      " - apriori  \n",
      " - first guess  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#measurement error covariance\n",
      "se=np.array([[1,0.,0],[0,1.,0],[0,0,10.]])\n",
      "#apriori error covariance\n",
      "sa=np.array([[3,0.],[0,10.]])/10.\n",
      "#apriori\n",
      "xa=np.array([2.,3.])\n",
      "#first guess\n",
      "fg=np.array([2.4,8.])\n",
      "#Inversion\n",
      "print inv_func(y,se=se,sa=sa,xa=xa,fg=fg,method=2,full=True)\n",
      "print inv_func(y,se=se,sa=sa,xa=xa,fg=fg,method=2,full=True).a\n",
      "#using fparams\n",
      "print inv_func(y,se=se,sa=sa,xa=xa,fg=fg,method=2,fparams=1.8)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "result(x=array([ 1.87476534,  2.69800201]), j=array([[ 0.47411053,  0.3262691 ],\n",
        "       [-2.63752755,  2.5074966 ],\n",
        "       [ 0.40012524, -5.41679554]]), conv=True, ni=3, g=array([[ 0.11020149, -0.16358149, -0.04988926],\n",
        "       [ 0.10281939,  0.13705105, -0.08469297]]), a=array([[ 0.46373642, -0.10398479],\n",
        "       [-0.34661596,  0.83596636]]), sr=array([[ 0.16087907,  0.10398479],\n",
        "       [ 0.10398479,  0.16403364]]), cost=0.39360704451601464)\n",
        "[[ 0.46373642 -0.10398479]\n",
        " [-0.34661596  0.83596636]]\n",
        "[ 2.17714998  2.01051616]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "You can also provide a function that calculates the jacobian. See *usage_with_ext_jacobian.ipnb*"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    }
   ],
   "metadata": {}
  }
 ]
}